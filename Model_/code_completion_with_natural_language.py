# -*- coding: utf-8 -*-
"""Code Completion with Natural Language.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fNdkHFFjS566vbQl_tVtMfiQJDnG_0k0
"""

import json
import random

def augment_code(code):
    # Convert the code to a list of lines
    lines = code.strip().split('\n')
    # Shuffle the order of lines randomly
    random.shuffle(lines)
    # Join the shuffled lines back to form the augmented code
    augmented_code = '\n'.join(lines)
    return augmented_code

def main():
    # Read the JSON data from the file
    with open('/content/codes.json', 'r') as file:
        data = json.load(file)

    # Perform data augmentation
    augmented_data = []
    for item in data:
        code = item['code']
        description = item['description']

        # Original data
        augmented_data.append({
            'code': code,
            'description': description
        })

        # Data augmentation by shuffling the code statements
        augmented_code = augment_code(code)
        augmented_data.append({
            'code': augmented_code,
            'description': description + " (Augmented)"
        })

    # Save the augmented data to a new JSON file
    with open('augmented_data.json', 'w') as outfile:
        json.dump(augmented_data, outfile, indent=2)

if __name__ == "__main__":
    main()

import json

# Read the JSON file
with open('/content/augmented_data.json', 'r') as file:
    data = json.load(file)

# Print the data
print(data)

import requests
from bs4 import BeautifulSoup

def scrape_github_repository(repository_url):
    # Send a GET request to the repository URL
    response = requests.get(repository_url)

    if response.status_code == 200:
        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the code snippets and descriptions on the repository page
        code_snippets = []
        descriptions = []

        code_elements = soup.find_all('pre', {'class': 'highlight'})
        description_elements = soup.find_all('p', {'class': 'commit-title'})

        for code_element, description_element in zip(code_elements, description_elements):
            code = code_element.text.strip()
            description = description_element.text.strip()

            code_snippets.append(code)
            descriptions.append(description)

        return code_snippets, descriptions
    else:
        print("Failed to fetch repository data.")
        return [], []

# Example usage
repository_url = "https://github.com/someuser/somerepo"
snippets, descriptions = scrape_github_repository(repository_url)

for snippet, description in zip(snippets, descriptions):
    print("Code Snippet:")
    print(snippet)
    print("Description:")
    print(description)
    print("-------------------------")

"""## Building with Dataset"""

!pip install datasets
!pip install humanize
import humanize
from datasets import load_dataset

import datasets
import pandas as pd
import humanize

# Check if the dataset is already downloaded, otherwise download it
try:
    dataset = datasets.load_dataset("code_search_net", name="python")
except FileNotFoundError:
    print("Downloading the dataset...")
    dataset = datasets.load_dataset("code_search_net", name="python")

# Get the number of rows and columns in the dataset
num_rows = len(dataset['train']) + len(dataset['test']) + len(dataset['validation'])
num_cols = len(dataset['train'][0])

print(f"Number of rows in the Python dataset: {num_rows}")
print(f"Number of columns in the Python dataset: {num_cols}")

# Combine all splits (train, test, validation) into one DataFrame
df = pd.concat([pd.DataFrame(dataset['train']),
                pd.DataFrame(dataset['test']),
                pd.DataFrame(dataset['validation'])])

# Save the combined data as a CSV file
csv_filename = "python_dataset.csv"
df.to_csv(csv_filename, index=False)

print(f"Python dataset saved as {csv_filename}")

dddf = pd.read_csv("/content/python_dataset.csv")

dddf.head()

dddf.describe(include='all')

df.to_csv('/content/drive/MyDrive/BIG_DATA/Language Model/Python_Codes_Desc.csv',index = False)

